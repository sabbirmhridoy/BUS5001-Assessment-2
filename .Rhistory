# Pruning the tree based on the complexity parameter
ptree <- prune(tree_model1, cp = tree_model1$cptable[which.min(tree_model1$cptable[,"xerror"]),"CP"])
# Plotting the original and pruned trees
par(mfrow = c(1, 2), mar = c(1, 1, 1, 1))  # Adjust plot margins
# Plot Original Tree
plot(tree_model1, main = "Original Tree", uniform = TRUE)
text(tree_model1, use.n = TRUE, cex = 0.6)
# Plot Pruned Tree
plot(ptree, main = "Pruned Tree", uniform = TRUE)
text(ptree, use.n = TRUE, cex = 0.6)
#2c Compare these decision tree models
# Load necessary library
if (!require(rpart)) install.packages("rpart")
library(rpart)
# Load the dataset
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Split data into training and validation sets
set.seed(123)
training_size <- floor(0.8 * nrow(data))
index <- sample(seq_len(nrow(data)), size = training_size)
training_data <- data[index, ]
validation_data <- data[-index, ]
# Train decision tree model
tree_model1 <- rpart(SalePrice ~ ., data = training_data, method = "anova")
# Exploring RMSE across different complexity parameters
cp_values <- seq(0.01, 0.1, by = 0.01)
rmse_values <- sapply(cp_values, function(cp) {
ptree_temp <- prune(tree_model1, cp = cp)
pred_temp <- predict(ptree_temp, newdata = validation_data)
sqrt(mean((validation_data$SalePrice - pred_temp)^2))
})
# Plotting RMSE across different cp values
plot(cp_values, rmse_values, type = "b", xlab = "Complexity Parameter (cp)", ylab = "RMSE",
main = "RMSE vs. Complexity Parameter", pch = 19, col = "blue", lwd = 2, cex = 1.2,
cex.main = 1.5, cex.lab = 1.2, cex.axis = 1.1, xlim = c(0, 0.1), ylim = range(rmse_values))
# Adding a grid for better readability
grid(nx = NULL, ny = NULL, col = "gray", lty = "dotted", lwd = par("lwd"))
#Part B
# Load necessary library
library(readr)
# Load the dataset from the given path
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Display the structure of the dataset to inspect variable types
str(data)
# Load necessary libraries
library(dplyr)
library(caret)  # For dummyVars function
# Assuming your data is stored in a dataframe called df
# List of continuous variables
continuous_vars <- c("LotArea","TotalBSF", "LowQualFinSF", "LivingArea", "PoolArea", "OpenPorchSF", "SalePrice")
# Calculate summary statistics
summary_stats <- sapply(df[continuous_vars], function(x) {
c(Mean = mean(x, na.rm = TRUE),
Median = median(x, na.rm = TRUE),
Max = max(x, na.rm = TRUE),
SD = sd(x, na.rm = TRUE))
})
print(names(df))
# Assuming your data is stored in a dataframe called df
# 1. Define the list of categorical variables
categorical_vars <- c("LotConfigFR2", "Utilities", "LotConfigFR3",
"LotShape", "Slope", "DwellClass2fmCon",
"DwellClassDuplex", "DwellClassTwnhs",
"DwellClassTwnhsE", "OverallQuality",
"OverallCondition", "ExteriorCondition",
"BasementCondition", "KitchenQuality",
"GarageTypeBasment", "GarageTypeBuiltIn",
"GarageTypeCarPort", "GarageTypeDetchd",
"PavedDrive")
# 2. Function to get counts for each category within each categorical variable
category_counts <- lapply(df[categorical_vars], function(x) {
as.data.frame(table(x))  # Convert the counts to a data frame
})
# Load necessary libraries for visualization
library(ggplot2)
library(dplyr)
# List of continuous variables
continuous_vars <- c("LotArea", "TotalBSF", "LowQualFinSF", "LivingArea",
"PoolArea", "OpenPorchSF", "SalePrice")
# 1. Boxplots for Visualizing Outliers
# Generate boxplots for each continuous variable
for (var in continuous_vars) {
p <- ggplot(df, aes(x = "", y = !!sym(var))) +  # x is set to "" for vertical plot
geom_boxplot(outlier.colour = "red", outlier.shape = 16,
outlier.size = 2, notch = FALSE) +
labs(title = paste("Boxplot of", var), y = var) +
theme_minimal() +
theme(axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank())
print(p)  # Explicitly print the plot
}
#Part B
#1a
# Load necessary library
library(readr)
# Load the dataset from the given path
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Display the structure of the dataset to inspect variable types
str(data)
#Part B
#1a
# Load necessary library
library(readr)
# Load the dataset from the given path
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Display the structure of the dataset to inspect variable types
str(data)
View(data)
#1bi
#One-Hot encoding
library(dplyr)
library(tidyr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# One-hot encoding the 'DwellClass' column
data_transformed <- data %>%
mutate(DwellClass = as.factor(DwellClass)) %>%
pivot_wider(names_from = DwellClass, values_from = DwellClass,
values_fill = list(DwellClass = 0), values_fn = list(DwellClass = length)) %>%
mutate(across(.cols = starts_with("DwellClass"), .fns = ~as.integer(. > 0)))
# View the transformed data
View(data_transformed)
head(data_transformed)
#1bii
#Label encoding
library(dplyr)
library(readr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Applying label encoding
data <- data %>%
mutate(DwellClass = as.factor(DwellClass),  # Ensure it's a factor if not already
DwellClass_encoded = as.integer(DwellClass))  # Convert factor levels to integers
# View the changes
head(data)
#1bii
#Label encoding
library(dplyr)
library(readr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Applying label encoding
data <- data %>%
mutate(DwellClass = as.factor(DwellClass),  # Ensure it's a factor if not already
DwellClass_encoded = as.integer(DwellClass))  # Convert factor levels to integers
# View the changes
head(data)
`
```{r}
library(dplyr)
library(tidyr)  # for pivot_wider
# Assuming 'property_type' is the correct column name
# Check if it exists in the dataset
if("property_type" %in% colnames(data)) {
# Proceed with one-hot encoding
data_transformed <- data %>%
mutate(across(.cols = c("property_type"), .fns = ~factor(.))) %>%
pivot_wider(names_from = property_type, values_from = property_type,
values_fill = list(property_type = 0), values_fn = list(property_type = length)) %>%
mutate(across(.cols = starts_with("property_type"), .fns = ~as.integer(. > 0)))
} else {
stop("Column 'property_type' not found in the dataset")
}
#2a (i)
#1. Calculating Summary Statistics for Continuous Variables
#For continuous variables like LotArea, YearBuilt, LivingArea, etc., we will calculate mean, median, max, and standard deviation.
library(dplyr)
library(readr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Selecting continuous variables for statistics
continuous_vars <- data %>%
select(LotArea, YearBuilt, LivingArea, TotalBSF, LowQualFinSF, GarageCars, PoolArea, OpenPorchSF, SalePrice) %>%
summarise_all(list(
mean = ~mean(.x, na.rm = TRUE),
median = ~median(.x, na.rm = TRUE),
max = ~max(.x, na.rm = TRUE),
sd = ~sd(.x, na.rm = TRUE)
))
# View the results
View(continuous_vars)
print(continuous_vars)
#2a (i)
#1. Calculating Summary Statistics for Continuous Variables
#For continuous variables like LotArea, YearBuilt, LivingArea, etc., we will calculate mean, median, max, and standard deviation.
library(dplyr)
library(readr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Selecting continuous variables for statistics
continuous_vars <- data %>%
select(LotArea, YearBuilt, LivingArea, TotalBSF, LowQualFinSF, GarageCars, PoolArea, OpenPorchSF, SalePrice) %>%
summarise_all(list(
mean = ~mean(.x, na.rm = TRUE),
median = ~median(.x, na.rm = TRUE),
max = ~max(.x, na.rm = TRUE),
sd = ~sd(.x, na.rm = TRUE)
))
# View the results
View(continuous_vars)
print(continuous_vars)
#2a (ii)
# 2. Count for Each Categorical Variable
#For categorical variables like LotShape, LandContour, DwellClass, etc., we will calculate the count of each category:
library(dplyr)
library(readr)
# Load the dataset
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Counting categories in each categorical variable individually and storing results
counts_LotShape <- data %>% count(LotShape)
counts_LandContour <- data %>% count(LandContour)
counts_Utilities <- data %>% count(Utilities)
counts_DwellClass <- data %>% count(DwellClass)
counts_GarageType <- data %>% count(GarageType)
# Print counts to console (or use View() if in RStudio to inspect)
print(counts_LotShape)
print(counts_LandContour)
print(counts_Utilities)
print(counts_DwellClass)
print(counts_GarageType)
# Combining all counts into one list for easier viewing
all_counts <- list(
LotShape = counts_LotShape,
LandContour = counts_LandContour,
Utilities = counts_Utilities,
DwellClass = counts_DwellClass,
GarageType = counts_GarageType
)
# Optionally, print or view the combined list
print(all_counts)
#3i
# Load required package
if (!requireNamespace("ggplot2", quietly = TRUE)) {
install.packages("ggplot2")
}
library(ggplot2)
# Load the dataset using base R's read.csv
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")  # Update the path as necessary
# Define continuous variables
continuous_vars <- c("LotArea", "YearBuilt", "LivingArea", "TotalBSF", "SalePrice")
# Initialize an empty list to store results for further analysis or saving
results <- list()
# Function to calculate skewness and kurtosis
calculate_stats <- function(x) {
n <- length(x)
mean_x <- mean(x, na.rm = TRUE)
sd_x <- sd(x, na.rm = TRUE)
skewness <- sum((x - mean_x)^3 / sd_x^3, na.rm = TRUE) / n
kurtosis <- sum((x - mean_x)^4 / sd_x^4, na.rm = TRUE) / n - 3
return(c(Skewness = skewness, Kurtosis = kurtosis))
}
# Iterate through each variable to analyze
for (var in continuous_vars) {
# Plot histogram
plot <- ggplot(data, aes_string(x = var)) +
geom_histogram(bins = 30, fill = "Orange", color = "black") +
labs(title = paste("Histogram of", var), x = var, y = "Frequency") +
theme_minimal()
# Print plot
print(plot)
# Calculate and print summary statistics
stats <- data %>%
filter(!is.na(.data[[var]])) %>%
summarise(
Mean = mean(.data[[var]]),
Median = median(.data[[var]]),
SD = sd(.data[[var]]),
Min = min(.data[[var]]),
Max = max(.data[[var]]),
Skewness = calculate_stats(.data[[var]])['Skewness'],
Kurtosis = calculate_stats(.data[[var]])['Kurtosis']
)
print(stats)
# Store results in the list
results[[var]] <- list(Plot = plot, Statistics = stats)
}
#4a. Which, if any, of the variables have missing values?
library(dplyr)
# Load the dataset
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")  # Update the path as necessary
# Check for missing values
missing_values <- sapply(data, function(x) sum(is.na(x)))
missing_values <- missing_values[missing_values > 0]  # Filter only variables with missing values
print(missing_values)
#4c. Apply the 3 methods of missing values and demonstrate the output.
#Method 1: Listwise Deletion
# Applying listwise deletion
data_listwise <- na.omit(data)
# Summary statistics after listwise deletion
summary_stats_listwise <- summary(data_listwise)
print(summary_stats_listwise)
#Method 2: Mean Imputation (for a numerical variable)
# Assuming 'LotArea' has missing values and is numerical
mean_LotArea <- mean(data$LotArea, na.rm = TRUE)
data_mean_imputation <- data
data_mean_imputation$LotArea[is.na(data_mean_imputation$LotArea)] <- mean_LotArea
# Summary statistics after mean imputation
summary_stats_mean <- summary(data_mean_imputation)
print(summary_stats_mean)
#Method 3: Adding a Placeholder for Categorical Variable
# Assuming 'BasementCondition' is categorical with missing values
data$BasementCondition[is.na(data$BasementCondition)] <- "Missing"
# Summary statistics after adding placeholder
summary_stats_placeholder <- summary(data)
print(summary_stats_placeholder)
#Which method of handling missing values is most suitable for this data set?
#Listwise Deletion: This might lead to a significant reduction in data, potentially biasing any analysis if missingness is not random.
#Mean Imputation: Preserves the dataset size but can reduce variability and impact correlations because it artificially inflates the number of observations at the mean.
#Placeholder for Categorical Variable: Useful for maintaining data integrity without assuming missingness can be represented by existing categories. It avoids the pitfalls of imputation but may introduce a new category that needs careful interpretation in predictive modeling.
#The choice depends on the nature of missingness and the importance of retaining full data for analysis. If the missing data are not MNAR (Missing Not At Random), then imputation might be suitable. However, if data integrity and accurate representation of missingness are crucial, using placeholders or more sophisticated imputation models might be preferred.
#4c. Apply the 3 methods of missing values and demonstrate the output.
#Method 1: Listwise Deletion
# Applying listwise deletion
data_listwise <- na.omit(data)
# Summary statistics after listwise deletion
summary_stats_listwise <- summary(data_listwise)
print(summary_stats_listwise)
view(summary_stats_listwise)
#4c. Apply the 3 methods of missing values and demonstrate the output.
#Method 1: Listwise Deletion
# Applying listwise deletion
data_listwise <- na.omit(data)
# Summary statistics after listwise deletion
summary_stats_listwise <- summary(data_listwise)
print(summary_stats_listwise)
View(summary_stats_listwise)
#Method 2: Mean Imputation (for a numerical variable)
# Assuming 'LotArea' has missing values and is numerical
mean_LotArea <- mean(data$LotArea, na.rm = TRUE)
data_mean_imputation <- data
data_mean_imputation$LotArea[is.na(data_mean_imputation$LotArea)] <- mean_LotArea
# Summary statistics after mean imputation
summary_stats_mean <- summary(data_mean_imputation)
print(summary_stats_mean)
#Method 3: Adding a Placeholder for Categorical Variable
# Assuming 'BasementCondition' is categorical with missing values
data$BasementCondition[is.na(data$BasementCondition)] <- "Missing"
# Summary statistics after adding placeholder
summary_stats_placeholder <- summary(data)
print(summary_stats_placeholder)
#Which method of handling missing values is most suitable for this data set?
#Listwise Deletion: This might lead to a significant reduction in data, potentially biasing any analysis if missingness is not random.
#Mean Imputation: Preserves the dataset size but can reduce variability and impact correlations because it artificially inflates the number of observations at the mean.
#Placeholder for Categorical Variable: Useful for maintaining data integrity without assuming missingness can be represented by existing categories. It avoids the pitfalls of imputation but may introduce a new category that needs careful interpretation in predictive modeling.
#The choice depends on the nature of missingness and the importance of retaining full data for analysis. If the missing data are not MNAR (Missing Not At Random), then imputation might be suitable. However, if data integrity and accurate representation of missingness are crucial, using placeholders or more sophisticated imputation models might be preferred.
#4c. Apply the 3 methods of missing values and demonstrate the output.
#Method 1: Listwise Deletion
# Applying listwise deletion
data_listwise <- na.omit(data)
# Summary statistics after listwise deletion
summary_stats_listwise <- summary(data_listwise)
print(summary_stats_listwise)
View(summary_stats_listwise)
#Method 2: Mean Imputation (for a numerical variable)
# Assuming 'LotArea' has missing values and is numerical
mean_LotArea <- mean(data$LotArea, na.rm = TRUE)
data_mean_imputation <- data
data_mean_imputation$LotArea[is.na(data_mean_imputation$LotArea)] <- mean_LotArea
# Summary statistics after mean imputation
summary_stats_mean <- summary(data_mean_imputation)
print(summary_stats_mean)
View(summary_stats_mean)
#Method 3: Adding a Placeholder for Categorical Variable
# Assuming 'BasementCondition' is categorical with missing values
data$BasementCondition[is.na(data$BasementCondition)] <- "Missing"
# Summary statistics after adding placeholder
summary_stats_placeholder <- summary(data)
print(summary_stats_placeholder)
#Which method of handling missing values is most suitable for this data set?
#Listwise Deletion: This might lead to a significant reduction in data, potentially biasing any analysis if missingness is not random.
#Mean Imputation: Preserves the dataset size but can reduce variability and impact correlations because it artificially inflates the number of observations at the mean.
#Placeholder for Categorical Variable: Useful for maintaining data integrity without assuming missingness can be represented by existing categories. It avoids the pitfalls of imputation but may introduce a new category that needs careful interpretation in predictive modeling.
#The choice depends on the nature of missingness and the importance of retaining full data for analysis. If the missing data are not MNAR (Missing Not At Random), then imputation might be suitable. However, if data integrity and accurate representation of missingness are crucial, using placeholders or more sophisticated imputation models might be preferred.
#4c. Apply the 3 methods of missing values and demonstrate the output.
#Method 1: Listwise Deletion
# Applying listwise deletion
data_listwise <- na.omit(data)
# Summary statistics after listwise deletion
summary_stats_listwise <- summary(data_listwise)
print(summary_stats_listwise)
View(summary_stats_listwise)
#Method 2: Mean Imputation (for a numerical variable)
# Assuming 'LotArea' has missing values and is numerical
mean_LotArea <- mean(data$LotArea, na.rm = TRUE)
data_mean_imputation <- data
data_mean_imputation$LotArea[is.na(data_mean_imputation$LotArea)] <- mean_LotArea
# Summary statistics after mean imputation
summary_stats_mean <- summary(data_mean_imputation)
print(summary_stats_mean)
View(summary_stats_mean)
#Method 3: Placeholder Values for Categorical Variable
# Assuming 'BasementCondition' is categorical with missing values
data$BasementCondition[is.na(data$BasementCondition)] <- "Missing"
# Summary statistics after adding placeholder
summary_stats_placeholder <- summary(data)
print(summary_stats_placeholder)
View(summary_stats_placeholder)
#Which method of handling missing values is most suitable for this data set?
#Listwise Deletion: This might lead to a significant reduction in data, potentially biasing any analysis if missingness is not random.
#Mean Imputation: Preserves the dataset size but can reduce variability and impact correlations because it artificially inflates the number of observations at the mean.
#Placeholder for Categorical Variable: Useful for maintaining data integrity without assuming missingness can be represented by existing categories. It avoids the pitfalls of imputation but may introduce a new category that needs careful interpretation in predictive modeling.
#The choice depends on the nature of missingness and the importance of retaining full data for analysis. If the missing data are not MNAR (Missing Not At Random), then imputation might be suitable. However, if data integrity and accurate representation of missingness are crucial, using placeholders or more sophisticated imputation models might be preferred.
#5a
# Load necessary packages, installing them if not already available
if (!requireNamespace("ggplot2", quietly = TRUE)) {
install.packages("ggplot2")
}
library(ggplot2)
if (!requireNamespace("reshape2", quietly = TRUE)) {
install.packages("reshape2")
}
library(reshape2)
# Load your data
# Make sure to update the path to where your dataset is actually located
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")  # Update the path as necessary
# Selecting only numeric columns for correlation analysis
numerical_data <- data[, sapply(data, is.numeric)]
# Calculate the correlation matrix, handling missing values by using complete observations
correlation_matrix <- cor(numerical_data, use = "complete.obs")
# Reshape the correlation matrix for visualization
melted_correlation_matrix <- melt(correlation_matrix)
names(melted_correlation_matrix) <- c("Variable1", "Variable2", "Correlation")
# Visualize the correlation matrix using ggplot2
cor_plot <- ggplot(data = melted_correlation_matrix, aes(x = Variable1, y = Variable2, fill = Correlation)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white",
midpoint = 0, limit = c(-1,1), space = "Lab",
name="Correlation") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
labs(title = "Correlation Matrix", x = "", y = "")
# Print the plot
print(cor_plot)
View(cor_plot)
#1ai. Build a Regression Model with Selected Variables
#With the selected variables based on previous analysis: GarageCars, TotalRmsAbvGrd, and FullBath, with SalePrice as the target variable.
# Load the dataset
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Build the linear regression model
model1 <- lm(SalePrice ~ GarageCars + TotalRmsAbvGrd + FullBath, data = data)
# Summary of the model to see coefficients and statistics
summary(model1)
View(model1)
#1ai. Build a Regression Model with Selected Variables
#With the selected variables based on previous analysis: GarageCars, TotalRmsAbvGrd, and FullBath, with SalePrice as the target variable.
# Load the dataset
data <- read.csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Build the linear regression model
model1 <- lm(SalePrice ~ GarageCars + TotalRmsAbvGrd + FullBath, data = data)
# Summary of the model to see coefficients and statistics
summary(model1)
#1 Stepwise Selection (both directions):
stepModel <- step(model1, direction = "both")
summary(stepModel)
#1b. Evaluate the Regression Model and Carry Out Feature Selection
#To improve the model, we can explore other combinations and use feature selection techniques like stepwise selection.
#Here are three models using different selection criteria:
#1 Stepwise Selection (both directions):
stepModel <- step(model1, direction = "both")
summary(stepModel)
#2 Adding interaction terms to see if they improve model performance:
model2 <- lm(SalePrice ~ GarageCars * TotalRmsAbvGrd * FullBath, data = data)
summary(model2)
#3 Using another subset of variables or transformations (e.g., square of rooms to capture nonlinear effects):
model3 <- lm(SalePrice ~ GarageCars + I(TotalRmsAbvGrd^2) + FullBath, data = data)
summary(model3)
#1c Compare These Regression Models Based on Evaluation Metrics
#To compare these models, by looking into metrics like R-squared, Adjusted R-squared, AIC, and BIC:
# Comparison of models based on summary statistics
comparison <- data.frame(
Model = c("Model 1", "Stepwise Model", "Model 2", "Model 3"),
R_Squared = c(summary(model1)$r.squared, summary(stepModel)$r.squared, summary(model2)$r.squared, summary(model3)$r.squared),
Adjusted_R = c(summary(model1)$adj.r.squared, summary(stepModel)$adj.r.squared, summary(model2)$adj.r.squared, summary(model3)$adj.r.squared),
AIC = AIC(model1, stepModel, model2, model3),
BIC = BIC(model1, stepModel, model2, model3)
)
# Print the model comparisons
print(comparison)
View(comparison)
#2b. Evaluate the Decision Tree Model and Carry Out Pruning
#To build better decision tree models to adjust parameters and prune the tree to avoid overfitting.
#Pruned Tree:
# Pruning the tree
# Load necessary library
library(rpart)
#1 Pruning the tree
ptree <- prune(tree_model1, cp = tree_model1$cptable[which.min(tree_model1$cptable[,"xerror"]),"CP"])
# Plot the pruned tree with adjusted parameters
plot(ptree, main = "Pruned Decision Tree", uniform = TRUE, cex = 0.5, xpd = NA)
text(ptree, use.n = TRUE, all = TRUE, cex = 0.5, xpd = NA)
#2 Tree with Different Complexity Parameters
# Build and plot a decision tree with specified complexity parameter (cp)
tree_model2 <- rpart(SalePrice ~ GarageCars + TotalRmsAbvGrd + FullBath, data = data, method = "anova", cp = 0.01)
plot(tree_model2, main = "Decision Tree with cp = 0.01", cex = 0.5, xpd = NA)
text(tree_model2, use.n = TRUE, all = TRUE, cex = 0.5, xpd = NA)
#3 Tree Using Different Minbucket Sizes
# Build and plot a decision tree with a specified minimum bucket size
tree_model3 <- rpart(SalePrice ~ GarageCars + TotalRmsAbvGrd + FullBath, data = data, method = "anova", minbucket = 10)
plot(tree_model3, main = "Decision Tree with minbucket = 10", cex = 0.5, xpd = NA)
text(tree_model3, use.n = TRUE, all = TRUE, cex = 0.5, xpd = NA)
# Install and load necessary libraries (if not already installed)
install.packages("rpart")
install.packages("rpart.plot")
#Part B
#1a
# Load necessary library
library(readr)
# Load the dataset from the given path
data <- read_csv("C:/Users/ijriv/OneDrive/Documents/Semester 2/BUS5PA - Predictive Analytics/Assessment 1/HousingValuation.csv")
# Display the structure of the dataset to inspect variable types
str(data)
